from moleculekit.molecule import Molecule
from moleculekit.tools.voxeldescriptors import getVoxelDescriptors, viewVoxelFeatures
from moleculekit.tools.atomtyper import prepareProteinForAtomtyping
from moleculekit.smallmol.smallmol import SmallMol
from moleculekit.home import home
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader, random_split
import re
import pandas as pd
import torch.optim as optim

"""
Attempting to build model in lines below; kind of shit so far
Inspiration coming from following sources:
- https://rosenfelder.ai/multi-input-neural-network-pytorch/
- https://software.acellera.com/docs/latest/moleculekit/tutorials/voxelization_tutorial.html?highlight=voxelization
- https://medium.datadriveninvestor.com/dual-input-cnn-with-keras-1e6d458cd979

"""

os.chdir('/home/users/jvs15/project-protein-fold/moleculekit')

xdat = np.load('xdata.npy', allow_pickle=True)
ydat = np.load('ydata.npy', allow_pickle=True)


class ProtDataset(Dataset):
    def __init__(self):
        self.x = torch.from_numpy(np.load('x_data.npy'))
        self.y = torch.from_numpy(np.load('y_data.npy'))
        self.n_pairs = len(self.y)

    def __len__(self):
        return self.n_pairs

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        lig = x[idx][0]
        prot = x[idx][1]
        y = y[idx]

        return lig, prot, y


def conv_block(input_size, output_size):
    block = nn.Sequential(
        nn.Conv3d(input_size, output_size, 5, stride=1, padding=1),
        nn.LeakyReLU(),
        nn.BatchNorm3d(output_size),
        nn.MaxPool3d(2)
        # Add dropout here later
    )
    return block

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Sequential(
            conv_block(8, 16),
            conv_block(16, 32),
            conv_block(32, 64))
        self.conv2 = nn.Sequential(
            conv_block(8, 32),
            conv_block(32, 64))

        self.flatten = nn.Flatten()
        self.LeakyReLU = nn.LeakyReLU()


    def forward(self, x1, x2):
        # Convolution blocks to be performed on the protein
        prot = self.conv1(x2)
        prot = self.flatten(prot)
        
        # Convolution blocks to be performed on the ligand
        lig = self.conv2(x1)
        lig = self.flatten(lig)

        print(prot.shape)
        print(lig.shape)
        hybrid = torch.cat((prot, lig), dim=1)
        hybrid = self.LeakyReLU(hybrid)

        return torch.sigmoid(hybrid)

model = Model()
#out = model.forward(lig_vox_t, prot_vox_t)

epochs = 100
lr = 0.1

criterion = nn.L1Loss()
optimizer = optim.Adam(model.parameters())

for epoch in range(epochs):
    running_loss = 0.0
    """
    for i, data in enumerate(dataloader_train, 0):
        model.train()
        # get the inputs; data is a list of [prot, lig, labels]
        prot, labels = data
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(prot.float())
        labels = labels.unsqueeze(1).float()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 10 == 0:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')
            running_loss = 0.0
    """


